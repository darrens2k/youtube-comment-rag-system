{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47431f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\d\\Documents\\GitHub\\youtube-comment-rag-system\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import googleapiclient\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "import re\n",
    "import ollama\n",
    "from langchain.schema import HumanMessage\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e17dae73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "As a text-based AI language model, I don't have any physical parameters or characteristics. However, I can provide information on the number of parameters in different contexts:\n",
      "\n",
      "1. Number of parameters in a linear regression model: In a linear regression model, the number of parameters (also known as coefficients) is equal to the number of independent variables plus one. For example, if you have two independent variables, the model will have three parameters (one for each independent variable and one for the intercept).\n",
      "2. Number of parameters in a neural network: The number of parameters in a neural network depends on the number of layers, the number of neurons in each layer, and the complexity of the connections between them. In general, the number of parameters in a neural network can be quite large, especially for deeper networks with more layers and more complex connections.\n",
      "3. Number of parameters in a statistical model: The number of parameters in a statistical model depends on the type of model and the number of observations. For example, in a linear regression model with n observations and p independent variables, the number of parameters is np. Similarly, in a logistic regression model with n observations and p independent variables, the number of parameters is np(1-p).\n",
      "4. Number of parameters in a machine learning algorithm: The number of parameters in a machine learning algorithm can vary depending on the specific algorithm and the size of the dataset. For example, the number of parameters in a decision tree model depends on the number of features used to train the tree, while the number of parameters in a support vector machine (SVM) model depends on the number of support vectors and the kernel function used.\n",
      "5. Number of parameters in a computer program: The number of parameters in a computer program can vary depending on the complexity of the program and the number of functions or subroutines used. For example, a simple program with a few functions may have only a few parameters, while a larger program with many more functions may have hundreds or thousands of parameters.\n",
      "\n",
      "I hope this helps! Let me know if you have any other questions.\n"
     ]
    }
   ],
   "source": [
    "response = ollama.generate(model='llama2:7B', prompt='how many parameters do you have')\n",
    "print(response['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "734aa82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To input a Pandas DataFrame into ChromaDB using Python, you can use the `chroma_db.load()` function. Here's an example of how to do this:\n",
      "```\n",
      "import pandas as pd\n",
      "from chroma_db import ChromaDB\n",
      "\n",
      "# Load the dataframe into a ChromaDB object\n",
      "db = ChromaDB()\n",
      "db.load(pd.DataFrame({'x': [1, 2, 3], 'y': [4, 5, 6]})\n",
      "```\n",
      "In this example, we first create a Pandas DataFrame with two columns (`x` and `y`) and three rows. We then pass this DataFrame to the `load()` function of the ChromaDB object, which will store the data in the ChromaDB database.\n",
      "\n",
      "You can also specify additional options when loading the DataFrame, such as the column names to use for the ChromaDB keys and values, like this:\n",
      "```\n",
      "db = ChromaDB()\n",
      "db.load(pd.DataFrame({'x': [1, 2, 3], 'y': [4, 5, 6]}), key_column='x', value_column='y')\n",
      "```\n",
      "This tells ChromaDB to use the `x` column as the keys and the `y` column as the values in the database.\n",
      "\n",
      "Alternatively, you can also load a DataFrame from a file using the `load_from_csv()` function:\n",
      "```\n",
      "import pandas as pd\n",
      "from chroma_db import ChromaDB\n",
      "\n",
      "# Load the dataframe from a CSV file\n",
      "db = ChromaDB()\n",
      "db.load_from_csv('data.csv')\n",
      "```\n",
      "This will load the DataFrame from the `data.csv` file and store it in the ChromaDB database.\n"
     ]
    }
   ],
   "source": [
    "response = ollama.generate(model='llama2:7B', prompt='how can I input a dataframe into chroma db on python')\n",
    "print(response['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae39b82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The context length refers to the number of words or characters that are considered when calculating the similarity between two texts. The context length can affect the accuracy of the text similarity measurement, as longer context lengths can capture more subtle similarities and variations in language use.\n",
      "\n",
      "Here are some common context lengths used in text similarity measures:\n",
      "\n",
      "1. Word-level: This is the most common context length, where the similarity between two texts is measured based on the number of shared words.\n",
      "2. Character-level: At this level, the similarity is measured based on the number of shared characters between two texts, without considering word boundaries.\n",
      "3. Context window: This involves dividing a text into a fixed-size context window and measuring the similarity between the window and another text. The size of the context window can vary, but common sizes include 10, 20, or 30 words.\n",
      "4. N-grams: This involves measuring the similarity between two texts based on a set of predefined n-grams (sequences of n items). For example, a measure might compare the frequency of co-occurring phrases in two texts.\n",
      "5. Sentence-level: This involves measuring the similarity between two texts based on the number of shared sentences or phrases.\n",
      "6. Paragraph-level: Similar to sentence-level, but at the level of paragraphs instead.\n",
      "7. Document-level: This involves measuring the similarity between two documents based on their overall structure and content, rather than just individual sentences or phrases.\n",
      "\n",
      "The choice of context length depends on the specific application and the type of analysis being performed. For example, in plagiarism detection, a shorter context length may be more appropriate to detect direct copying, while a longer context length may be more suitable for identifying subtle instances of plagiarism.\n"
     ]
    }
   ],
   "source": [
    "response = ollama.generate(model='llama2:7B', prompt='what is your context length')\n",
    "print(response['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ade55133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I can generate text responses of varying lengths, but the maximum length of a response I can provide is limited by the platform's character limit. On most platforms, my response will be limited to around 200-300 characters. However, if you have a longer message or request, feel free to ask and I will do my best to accommodate it!\n"
     ]
    }
   ],
   "source": [
    "response = ollama.generate(model='llama2:7B', prompt='how long of a response can you type out')\n",
    "print(response['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb33819c",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"AIzaSyB6-Tl-ScLoYTiFZLweVE8FebvY9ghrjqc\"\n",
    "video_id = [\"oqL7Ke4O3fg\", \"Tl8RS0sR-qA\", \"u4LUix-BU0s\", \"FHsONupIdlo\", \"KZeIEiBrT_w\", \"A5w-dEgIU1M\", \"bBC-nXj3Ng4\", \"vhzYhq0oTu8\", \"8Gm7kSUkBAk\", \"IF8YNn4v-y4\", \"aiv6kJ7eJ5U\"]\n",
    "# create a list dictionary that will store the output\n",
    "output = []\n",
    "# use api key to create youtube object\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c31663ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to call api and produce a list of comment-reply pairs\n",
    "\n",
    "# video == specify id of the video to get comments from\n",
    "# amount_of_comments == amount of comments to retrieve from current video\n",
    "# output == list item to store comment-reply pairs (pass as argument so function can continuously add to it)\n",
    "def call_youTube_API(video, amount_of_comments, output):\n",
    "\n",
    "    # call api to get comments on a particular video using video id\n",
    "    # order comments by relevance, popular comments are more likely to have replies\n",
    "    apiCall = youtube.commentThreads().list(part=[\"snippet\",\"replies\"], videoId=video, maxResults=amount_of_comments, order=\"relevance\").execute()\n",
    "    \n",
    "    # iterate through the comments the api returned\n",
    "    for i in range(len(apiCall[\"items\"])):\n",
    "        \n",
    "        # get comment text\n",
    "        textOutput = apiCall[\"items\"][i][\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
    "        \n",
    "        # get count of replies\n",
    "        replyCount = apiCall[\"items\"][i][\"snippet\"][\"totalReplyCount\"]\n",
    "        \n",
    "        if replyCount > 0:\n",
    "            \n",
    "            # get list of all the returned replies (api usually returns 5 replies)\n",
    "            replies = apiCall[\"items\"][i][\"replies\"][\"comments\"]\n",
    "\n",
    "            # get the likes per reply\n",
    "            likes = []\n",
    "            for reply in replies:\n",
    "                likes.append(reply[\"snippet\"][\"likeCount\"])\n",
    "                \n",
    "            # get index of comment with most likes\n",
    "            maxIndex = likes.index(max(likes))\n",
    "            # print comment with most likes\n",
    "            mostLikedReplyText = replies[maxIndex][\"snippet\"][\"textDisplay\"]\n",
    "\n",
    "            # save comment text and most liked reply text to output list dictionary\n",
    "            output.append({\"comment\":textOutput, \"reply\":mostLikedReplyText})\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "131758a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call function to get comments through all videos\n",
    "for video in video_id:\n",
    "    \n",
    "    call_youTube_API(video, 250, output)\n",
    "\n",
    "# convert list of dictionaries to dataframe\n",
    "output = pd.DataFrame(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c4f6f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some data cleaning\n",
    "\n",
    "# in order: remove links, html tags, special characters and punctuation, emojis\n",
    "output['comment'] = output['comment'].astype(str) \\\n",
    "    .str.replace(r\"http\\S+|www\\S+|https\\S+\", \"\", regex=True) \\\n",
    "    .str.replace(r\"<.*?>\", \"\", regex=True) \\\n",
    "    .str.replace(r\"[^\\w\\s]\", \"\", regex=True) \\\n",
    "    .str.replace(r\"[\\U00010000-\\U0010ffff]\", \"\", regex=True)\n",
    "\n",
    "output['reply'] = output['reply'].astype(str) \\\n",
    "    .str.replace(r\"http\\S+|www\\S+|https\\S+\", \"\", regex=True) \\\n",
    "    .str.replace(r\"<.*?>\", \"\", regex=True) \\\n",
    "    .str.replace(r\"[^\\w\\s]\", \"\", regex=True) \\\n",
    "    .str.replace(r\"[\\U00010000-\\U0010ffff]\", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "786165d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize an instance of the database\n",
    "# persist ensures that the database is saved to the computer so I can reference it in other scripts\n",
    "database = chromadb.PersistentClient(path=\"./youtube_comment_database\")\n",
    "# create a collection (group of documents and their embeddings)\n",
    "collection = database.create_collection(name=\"youtube_comments\")\n",
    "\n",
    "# import sentence embedder from huggingface\n",
    "# using all-MiniLM-L6-v2 since llama2:7B doesn't have an encoder and this one is light enough for me to run\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e31c9ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepping data for embedding model\n",
    "\n",
    "# only going to embed the comments\n",
    "# users will prompt the llm with a comment and the llm will draft a reply to the comment based on the comment-reply pairs the semantic search returns\n",
    "# when the semantic search is happening, we should only be searching the comments, I want to see the comment-reply pairs for the most similar comments\n",
    "# therefore the replies will be stored as metadata in the database while the only the comments will be embedded\n",
    "\n",
    "# convert dataframe items to lists\n",
    "comments = output[\"comment\"].to_list()\n",
    "replies = output[\"reply\"].to_list()\n",
    "\n",
    "# convert replies to list of dictionaries so I can pass it as metadata\n",
    "replies_dict = [{\"reply\":reply} for reply in replies]\n",
    "\n",
    "# embed comments\n",
    "encoded_comments = embedding_model.encode(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44e5bf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add data into database\n",
    "collection.add(\n",
    "    ids=[str(i) for i in range(len(comments))],\n",
    "    embeddings=encoded_comments,\n",
    "    documents=comments,\n",
    "    metadatas=replies_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b84f47b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['611', '7', '10', '1', '0']],\n",
       " 'embeddings': None,\n",
       " 'documents': [['Well I don39t care whatever others say My Honda Civic puts a smile on my face everytime I drive it One day hopefully I39ll put my hands on an Impreza but for now my Civic is perfect',\n",
       "   'Just sold a 2024 EX Civic and leased a 2025 premium 3 and man this thing is in a different league than the civic Dont listen to anyone that says but it has a torsion beam rear suspension They dont understand Mazda engineering',\n",
       "   'Mazdas are great cars very under appreciated ',\n",
       "   'As a current gen Mazda 3 owner this makes me very happy',\n",
       "   'When most companies make phones with wheels Mazda does the opposite I love it']],\n",
       " 'uris': None,\n",
       " 'included': ['metadatas', 'documents', 'distances'],\n",
       " 'data': None,\n",
       " 'metadatas': [[{'reply': 'I recently got a Civic 11th gen and its amazing Note that I never drove RWD before so I cant directly compare but the Civic is awesome in corners Same as you always got a big smile driving it Very fun and useful car'},\n",
       "   {'reply': 'ljohnson9440 nah this mazda 3 makes the civic looks cheap'},\n",
       "   {'reply': 'hulamanta3002 Good thing they39re not I guess'},\n",
       "   {'reply': 'Larger NA 4 cylinder and a nonCVT trans designed in Japan Yep  those are the reasons we bought one'},\n",
       "   {'reply': 'r3games1985what the f are you doing on a car that needs ChatGPT other than being a poser Cars are for driving not AI cringe'}]],\n",
       " 'distances': [[0.6557145118713379,\n",
       "   0.7328620553016663,\n",
       "   0.7562297582626343,\n",
       "   0.7603638768196106,\n",
       "   0.7749416828155518]]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# technically this file would end here\n",
    "# in the next file I will load the database and the model and then query it\n",
    "# doing a test query here though\n",
    "\n",
    "query = \"the mazda civic sucks\"\n",
    "\n",
    "# need to encode the query with the embedder\n",
    "encoded_query = embedding_model.encode(query)\n",
    "\n",
    "# search the database using the encoded query (semantic search)\n",
    "# distance metric is cosine similarity by default, need to set it when I set up the collection\n",
    "semantic_search_results = collection.query(query_embeddings=encoded_query, n_results=5)\n",
    "\n",
    "semantic_search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4ce2d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate data pipeline for later use\n",
    "# ingest comments that do and do not have more than 100 likes\n",
    "\n",
    "# video == specify id of the video to get comments from\n",
    "# amount_of_comments == amount of comments to retrieve from current video\n",
    "# output == list item to store comment-like pairs (pass as argument so function can continuously add to it)\n",
    "def call_youTube_API_likes(video, amount_of_comments, output):\n",
    "\n",
    "    # call api to get comments on a particular video using video id\n",
    "    # order comments by relevance, popular comments are more likely to have replies\n",
    "    apiCall = youtube.commentThreads().list(part=[\"snippet\",\"replies\"], videoId=video, maxResults=amount_of_comments, order=\"relevance\").execute()\n",
    "    \n",
    "    # iterate through the comments the api returned\n",
    "    for i in range(len(apiCall[\"items\"])):\n",
    "        \n",
    "        # get comment text\n",
    "        textOutput = apiCall[\"items\"][i][\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
    "        \n",
    "        # get count of likes\n",
    "        likeCount = apiCall[\"items\"][i][\"snippet\"][\"topLevelComment\"][\"snippet\"][\"likeCount\"]\n",
    "        \n",
    "        output.append({\"comment\":textOutput,\"like_count\":likeCount})\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07a0ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
