{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47431f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\d\\Documents\\GitHub\\youtube-comment-rag-system\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import googleapiclient\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "import re\n",
    "import ollama\n",
    "from langchain.schema import HumanMessage\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "786165d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize an instance of the database\n",
    "# persist ensures that the database is saved to the computer so I can reference it in other scripts\n",
    "database = chromadb.PersistentClient(path=\"./youtube_comment_database\")\n",
    "# create a collection (group of documents and their embeddings)\n",
    "collection = database.create_collection(name=\"youtube_comments\")\n",
    "\n",
    "# import sentence embedder from huggingface\n",
    "# using all-MiniLM-L6-v2 since llama2:7B doesn't have an encoder and this one is light enough for me to run\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb33819c",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"\"\n",
    "# video id's for videos about the civic si\n",
    "video_id = [\"RrZSuz-e9NY\", \"gGmdz9tA1Y8\", \"rGMWjQX5LG8\", \"nafje4-tv-w\", \"JcvQC0eYwJA\", \"DkW0Fr5KGf0\", \"ezgZCGM-_bg\", \"f6WAqT6073w\", \"F8IEZHeycS4\", \"VY91tZ3m-qU\", \"wczsTzaIgcE\", \"1h4MB5K_w1I\", \"JOp1xZrbuQM\", \"_e5mIqafwMA\", \"evTLpZZp6R0\", \"pUTj3C-Owx8\"]\n",
    "# create a list dictionary that will store the output\n",
    "output = []\n",
    "# use api key to create youtube object\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "046d8331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build function to call youtube api to go through as many pages of comments as possible per youtube video\n",
    "# video == id of particular video\n",
    "# output == output list\n",
    "# comments_to_view == number of comments model will go through per video (not all will have replies)\n",
    "def getCommentsPerVideo(video, output, comments_to_view=2000):\n",
    "    # parameter to specify the next page of comments, must be None for the first page\n",
    "    # each subsequent api call provides to value to load the next page\n",
    "    nextPageToken=None\n",
    "\n",
    "    # lets say we want no more than 1000 comments per video, each api call can get up to 100 comments\n",
    "    # so we will call the api in a 10 iteration loop and exit early if the nextPageToken is not given (means we already went through all comments)\n",
    "    # realistically most comments won't have a reply, so by iterating through 2000 we will get between 500 and 1000 usable data entries per video\n",
    "    for i in range(comments_to_view//100):\n",
    "\n",
    "        apiCall = youtube.commentThreads().list(part=[\"snippet\",\"replies\"], videoId=video, maxResults=100, order=\"relevance\", pageToken=nextPageToken).execute()\n",
    "\n",
    "        # iterate through the API response to save all comment-reply pairs (ignore comments that don't have any replies)\n",
    "        # iterate through the comments the api returned\n",
    "        for j in range(len(apiCall[\"items\"])):\n",
    "            \n",
    "            # get comment text\n",
    "            textOutput = apiCall[\"items\"][j][\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
    "            \n",
    "            # get count of replies\n",
    "            replyCount = apiCall[\"items\"][j][\"snippet\"][\"totalReplyCount\"]\n",
    "            \n",
    "            if replyCount > 0:\n",
    "                \n",
    "                # get list of all the returned replies (api usually returns 5 replies)\n",
    "                replies = apiCall[\"items\"][j][\"replies\"][\"comments\"]\n",
    "\n",
    "                # get the likes per reply\n",
    "                likes = []\n",
    "                for reply in replies:\n",
    "                    likes.append(reply[\"snippet\"][\"likeCount\"])\n",
    "                    \n",
    "                # get index of comment with most likes\n",
    "                maxIndex = likes.index(max(likes))\n",
    "                # get reply with most likes\n",
    "                mostLikedReplyText = replies[maxIndex][\"snippet\"][\"textDisplay\"]\n",
    "\n",
    "                # save comment text and most liked reply text to output list dictionary\n",
    "                output.append({\"comment\":textOutput, \"reply\":mostLikedReplyText})\n",
    "            \n",
    "            # update next page token\n",
    "            next_page_token = apiCall.get(\"nextPageToken\")\n",
    "            if not next_page_token:\n",
    "                break\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "# build function to get comments for all youtube videos specified\n",
    "# videos == list of video IDs\n",
    "def fetchYouTubeComments(videos):\n",
    "\n",
    "    # define output list\n",
    "    output = []\n",
    "\n",
    "    # iterate through list of video IDs\n",
    "    for video in videos:\n",
    "        \n",
    "        output = getCommentsPerVideo(video, output, comments_to_view=5000)\n",
    "\n",
    "    return output\n",
    "\n",
    "output = fetchYouTubeComments(video_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8eb36ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for data cleaning\n",
    "# accepts the output of the fetchYouTubeComments function\n",
    "\n",
    "def cleanData(output):\n",
    "\n",
    "    # convert list of dictionaries to dataframe\n",
    "    output = pd.DataFrame(output)\n",
    "\n",
    "    # drop any duplicate entries we have\n",
    "    output.drop_duplicates(inplace=True)\n",
    "\n",
    "    # in order: remove links, html tags, special characters and punctuation, emojis\n",
    "    output['comment'] = output['comment'].astype(str) \\\n",
    "        .str.replace(r\"http\\S+|www\\S+|https\\S+\", \"\", regex=True) \\\n",
    "        .str.replace(r\"<.*?>\", \"\", regex=True) \\\n",
    "        .str.replace(r\"[^\\w\\s]\", \"\", regex=True) \\\n",
    "        .str.replace(r\"[\\U00010000-\\U0010ffff]\", \"\", regex=True)\n",
    "\n",
    "    output['reply'] = output['reply'].astype(str) \\\n",
    "        .str.replace(r\"http\\S+|www\\S+|https\\S+\", \"\", regex=True) \\\n",
    "        .str.replace(r\"<.*?>\", \"\", regex=True) \\\n",
    "        .str.replace(r\"[^\\w\\s]\", \"\", regex=True) \\\n",
    "        .str.replace(r\"[\\U00010000-\\U0010ffff]\", \"\", regex=True)\n",
    "    \n",
    "    return output\n",
    "\n",
    "output = cleanData(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6581a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 509 entries, 0 to 23979\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   comment  509 non-null    object\n",
      " 1   reply    509 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 11.9+ KB\n"
     ]
    }
   ],
   "source": [
    "output.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9306ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to add data into vector database\n",
    "# accepts output from the cleanData function\n",
    "\n",
    "def uploadToVectorDB(df):\n",
    "\n",
    "    # initialize an instance of the database\n",
    "    # persist ensures that the database is saved to the computer so I can reference it in other scripts\n",
    "    database = chromadb.PersistentClient(path=\"./youtube_comment_database\")\n",
    "    # create a collection (group of documents and their embeddings)\n",
    "    collection = database.get_or_create_collection(name=\"youtube_comments\")\n",
    "\n",
    "    # import sentence embedder from huggingface\n",
    "    # using all-MiniLM-L6-v2 since llama2:7B doesn't have an encoder and this one is light enough for me to run\n",
    "    embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    # prepping data for embedding model\n",
    "\n",
    "    # only going to embed the comments\n",
    "    # users will prompt the llm with a comment and the llm will draft a reply to the comment based on the comment-reply pairs the semantic search returns\n",
    "    # when the semantic search is happening, we should only be searching the comments, I want to see the comment-reply pairs for the most similar comments\n",
    "    # therefore the replies will be stored as metadata in the database while the only the comments will be embedded\n",
    "\n",
    "    # convert dataframe items to lists\n",
    "    comments = df[\"comment\"].to_list()\n",
    "    replies = df[\"reply\"].to_list()\n",
    "\n",
    "    # convert replies to list of dictionaries so I can pass it as metadata\n",
    "    replies_dict = [{\"reply\":reply} for reply in replies]\n",
    "\n",
    "    # embed comments\n",
    "    encoded_comments = embedding_model.encode(comments)\n",
    "\n",
    "    # add data into database\n",
    "    collection.add(\n",
    "        ids=[str(i) for i in range(len(comments))],\n",
    "        embeddings=encoded_comments,\n",
    "        documents=comments,\n",
    "        metadatas=replies_dict\n",
    "    )\n",
    "\n",
    "    return\n",
    "\n",
    "uploadToVectorDB(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4349028f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed the prompt\n",
    "# load client\n",
    "database = chromadb.PersistentClient(path=\"../Data Collection/youtube_comment_database\")\n",
    "# get collection\n",
    "collection = database.get_or_create_collection(name=\"youtube_comments\")\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "prompt=\"i love the civic\"\n",
    "promptEncoded = embedding_model.encode(prompt)\n",
    "\n",
    "# search the database using the encoded query and get 5 most related comment-reply pairs\n",
    "# distance metric is cosine similarity by default, need to set it when I set up the collection\n",
    "semantic_search_results = collection.query(query_embeddings=promptEncoded, n_results=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7dd8abb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['366', '128', '46', '144', '344', '300']],\n",
       " 'embeddings': None,\n",
       " 'documents': [['The civic has always been pretty good to great I just wish it wasn39t getting bigger and bigger every generation',\n",
       "   'The latest generation of Civic looks so nice',\n",
       "   'I feel right in saying this generation makes me like the Civic as someone who39s never owned one',\n",
       "   'The Civic looks so much better to me than the CorollaCamry competitors',\n",
       "   'You guys rock  I love watching your Honda videos and listening to your thoughts on the new Civic Si  I think this is the best Civic Si Honda has ever made',\n",
       "   'Wow I remember when the Civic was a nifty little crackerbox that was a blast to drive They were a blast There was a station wagon too']],\n",
       " 'uris': None,\n",
       " 'included': ['metadatas', 'documents', 'distances'],\n",
       " 'data': None,\n",
       " 'metadatas': [[{'reply': 'I know this civic is 4 INCHES longer than the civic 25 years ago'},\n",
       "   {'reply': 'insid3493 Spot on dude IMO modern Japanese styling is way above anyone else at the moment and Honda is definitely at the forefront of that'},\n",
       "   {'reply': 'A I could be in the minority here but I think the new Civic and new WRX look better than their previous gens This is coming from a guy that has never owned either but now have my attention'},\n",
       "   {'reply': 'accord as well'},\n",
       "   {'reply': 'Thanks Jason'},\n",
       "   {'reply': 'I used to have one of those 3G Civic hatchback wagons that I used on the daily for a cheap to and from work runner It got great MPG39s and like you said was a total blast to drive Of course I did a complete devac on it and ran a Weber carb setup that really howled and made quite a bit more pep in its step I also liked the fact that being a hatchie it had more room in back and wasn39t just another Civic too'}]],\n",
       " 'distances': [[0.46261340379714966,\n",
       "   0.48953962326049805,\n",
       "   0.5456717014312744,\n",
       "   0.5704057216644287,\n",
       "   0.5824179649353027,\n",
       "   0.6023052334785461]]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6d85117",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiCall = youtube.commentThreads().list(part=[\"snippet\",\"replies\"], videoId=video_id[0], maxResults=250, order=\"relevance\", pageToken=\"Z2V0X3JhbmtlZF9zdHJlYW1zLS1Db0lFQ0lBRUZSZTMwVGdhOXdNSzhnTUkyRjhRZ0FRWUJ5TG5BNHVZSDYycW1xSW9Jd1l5b2lBekxobkw3VDBrbkRtMTd3MUJEQXN4UmRGc3NJQ2Mxd211TVZzaEtUOFVLMmpseFRlcjdDbzFHTWJqMDB4RXhseFEyV3NpRkZwTHJtVm9hNGFrWFVubzZHa29tenZyR01vVk1wdmVxRVF5TEozMGRHc0wxQkdzcENTa285QmZSUERzZ0MxdzM4QkVyU2FMemhYR085VWgyRUhwQ2lHb2RRMXlWenZqZFRtTEo0eEdFRGN3UzRoRjlnbVBCbzlrd1ViYWQ4bzd5RkxSeE42S3h3U1hNQmNHSGJHQzRCU25vcEo1NWhoaWFCVVcySldicFljdGtqN25PME1DY3k2VTl1T0JVOGlPZEVKcEFZX1BUY0VhczNKcUZIRlJSRkstVW84d0tteGQteFRoWGk1Rkp3S1RCNEdJbFZLTzBrVGF3Y3hRaFNIRlcxUkZjSTIxcjNVWnFycXBLNmRFa2JCUmRRTWZhSlVjNkw4a19JdzlYUE12RkJONEZiNGlBVlhRbkl2QWhFTG8zaXZIRWxVSklRZ3kwdEI5LW14V1ZhdFVsWmxnaWd6X3h6aGozem90Q0Niemh1aWs2ZnQ5UTJraWRqY09WWko1dExBc3M0clpyb0xST0lHbVM5cXJYUFlVU2xVNjQtTmh6Unh4aVhJX2l5TkpJRUxVekRjWFl6Rnd0aEFsdFhwSkZwblhINlFIVnRHc1Q0U1NpWnJha3FBUHFLZ0lzZ1YxZ2pnNEVFd0hoZ21qMHRNc1lzZTRzQlBCVFJvZE1TaU5yTlpCZHhHNXhHaUp0ZHdoVTdHaWJTRHJPRTVnblEyWWNVN25EU1F4YlZKQkJpWmhlZ0FRWkJJSENJVWdFR1FZQVJJRkNLZ2dHQUFTQlFpSUlCZ0FFZ1VJaHlBWUFCSUhDSVFnRUVJWUFSSUZDSWtnR0FB\").execute()\n",
    "\n",
    "# iterate through the comments the api returned\n",
    "for i in range(len(apiCall[\"items\"])):\n",
    "    \n",
    "    # get comment text\n",
    "    textOutput = apiCall[\"items\"][i][\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
    "    \n",
    "    # get count of replies\n",
    "    replyCount = apiCall[\"items\"][i][\"snippet\"][\"totalReplyCount\"]\n",
    "    \n",
    "    if replyCount > 0:\n",
    "        \n",
    "        # get list of all the returned replies (api usually returns 5 replies)\n",
    "        replies = apiCall[\"items\"][i][\"replies\"][\"comments\"]\n",
    "\n",
    "        # get the likes per reply\n",
    "        likes = []\n",
    "        for reply in replies:\n",
    "            likes.append(reply[\"snippet\"][\"likeCount\"])\n",
    "            \n",
    "        # get index of comment with most likes\n",
    "        maxIndex = likes.index(max(likes))\n",
    "        # print comment with most likes\n",
    "        mostLikedReplyText = replies[maxIndex][\"snippet\"][\"textDisplay\"]\n",
    "\n",
    "        # save comment text and most liked reply text to output list dictionary\n",
    "        output.append({\"comment\":textOutput, \"reply\":mostLikedReplyText})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c31663ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to call api and produce a list of comment-reply pairs\n",
    "\n",
    "# video == specify id of the video to get comments from\n",
    "# amount_of_comments == amount of comments to retrieve from current video\n",
    "# output == list item to store comment-reply pairs (pass as argument so function can continuously add to it)\n",
    "def call_youTube_API(video, amount_of_comments, output):\n",
    "\n",
    "    # call api to get comments on a particular video using video id\n",
    "    # order comments by relevance, popular comments are more likely to have replies\n",
    "    apiCall = youtube.commentThreads().list(part=[\"snippet\",\"replies\"], videoId=video, maxResults=amount_of_comments, order=\"relevance\").execute()\n",
    "    \n",
    "    # iterate through the comments the api returned\n",
    "    for i in range(len(apiCall[\"items\"])):\n",
    "        \n",
    "        # get comment text\n",
    "        textOutput = apiCall[\"items\"][i][\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
    "        \n",
    "        # get count of replies\n",
    "        replyCount = apiCall[\"items\"][i][\"snippet\"][\"totalReplyCount\"]\n",
    "        \n",
    "        if replyCount > 0:\n",
    "            \n",
    "            # get list of all the returned replies (api usually returns 5 replies)\n",
    "            replies = apiCall[\"items\"][i][\"replies\"][\"comments\"]\n",
    "\n",
    "            # get the likes per reply\n",
    "            likes = []\n",
    "            for reply in replies:\n",
    "                likes.append(reply[\"snippet\"][\"likeCount\"])\n",
    "                \n",
    "            # get index of comment with most likes\n",
    "            maxIndex = likes.index(max(likes))\n",
    "            # print comment with most likes\n",
    "            mostLikedReplyText = replies[maxIndex][\"snippet\"][\"textDisplay\"]\n",
    "\n",
    "            # save comment text and most liked reply text to output list dictionary\n",
    "            output.append({\"comment\":textOutput, \"reply\":mostLikedReplyText})\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "131758a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call function to get comments through all videos\n",
    "for video in video_id:\n",
    "    \n",
    "    call_youTube_API(video, 250, output)\n",
    "\n",
    "# convert list of dictionaries to dataframe\n",
    "output = pd.DataFrame(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c4f6f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some data cleaning\n",
    "\n",
    "# in order: remove links, html tags, special characters and punctuation, emojis\n",
    "output['comment'] = output['comment'].astype(str) \\\n",
    "    .str.replace(r\"http\\S+|www\\S+|https\\S+\", \"\", regex=True) \\\n",
    "    .str.replace(r\"<.*?>\", \"\", regex=True) \\\n",
    "    .str.replace(r\"[^\\w\\s]\", \"\", regex=True) \\\n",
    "    .str.replace(r\"[\\U00010000-\\U0010ffff]\", \"\", regex=True)\n",
    "\n",
    "output['reply'] = output['reply'].astype(str) \\\n",
    "    .str.replace(r\"http\\S+|www\\S+|https\\S+\", \"\", regex=True) \\\n",
    "    .str.replace(r\"<.*?>\", \"\", regex=True) \\\n",
    "    .str.replace(r\"[^\\w\\s]\", \"\", regex=True) \\\n",
    "    .str.replace(r\"[\\U00010000-\\U0010ffff]\", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e31c9ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepping data for embedding model\n",
    "\n",
    "# only going to embed the comments\n",
    "# users will prompt the llm with a comment and the llm will draft a reply to the comment based on the comment-reply pairs the semantic search returns\n",
    "# when the semantic search is happening, we should only be searching the comments, I want to see the comment-reply pairs for the most similar comments\n",
    "# therefore the replies will be stored as metadata in the database while the only the comments will be embedded\n",
    "\n",
    "# convert dataframe items to lists\n",
    "comments = output[\"comment\"].to_list()\n",
    "replies = output[\"reply\"].to_list()\n",
    "\n",
    "# convert replies to list of dictionaries so I can pass it as metadata\n",
    "replies_dict = [{\"reply\":reply} for reply in replies]\n",
    "\n",
    "# embed comments\n",
    "encoded_comments = embedding_model.encode(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44e5bf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add data into database\n",
    "collection.add(\n",
    "    ids=[str(i) for i in range(len(comments))],\n",
    "    embeddings=encoded_comments,\n",
    "    documents=comments,\n",
    "    metadatas=replies_dict\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
