import ollama

"""
LLM Interface Script

This module defines a function for generating YouTube comment replies using a locally
hosted Large Language Model (LLM) through the Ollama API. It performs lightweight
prompt engineering by constructing a context-rich input prompt that guides the model
to generate relevant, natural responses in the style of real YouTube interactions.

Function:
----------
callLLM(comments: list[str], replies: list[list[str]], prompt: str) -> str
    - Builds a pre-prompt with example commentâ€“reply pairs to condition the model.
    - Provides clear behavioral instructions for tone, structure, and content.
    - Appends the user's comment to request a tailored reply.
    - Calls the 'llama2:7B' model through the Ollama API using `generate()`.
    - Returns the generated reply text.

Parameters:
------------
comments : list of str
    Example YouTube comments used to provide conversational context.
replies : list of list of str
    Corresponding replies for each comment; only the first element of each list is used.
prompt : str
    The new user comment for which the model should generate a reply.

Returns:
---------
str
    The text response generated by the LLM.

Dependencies:
--------------
- ollama (local LLM API interface)
"""

# function to call LLM and get results
# also perform pre-prompting / prompt engineering
def callLLM(comments, replies, prompt):

    # write out the pre-prompt text for the llm
    pre_prompt = """
    You are a bot whose purpose is to reply to YouTube comments about the Honda Civic Si.
    You will be provided with examples of comment and reply pairings. Do not simply regurgitate these replies, use them as inspiration.
    Do not include special characters or usernames in your response. Do not prompt user for more information.
    Use only the information provided in the sample comment reply pairs to create your response.
    Respond by restating the comment and then your reply.
    Here are your samples:
    """

    # add in the comment and reply pairs
    for i in range(len(comments)):
        pre_prompt += "Comment " + str(i + 1) + ": " + comments[i] + "\nReply " + str(i + 1) + ": " + replies[i][0] + "\n\n"

    # add in the comment from user for llm to reply to
    pre_prompt += "\nDraft a reply to this comment: \n" + prompt

    # call llm and get response
    # using generate and not chat because don't need to have a conversation, just a response to the initial prompt
    response = ollama.generate(model='llama2:7B', prompt=pre_prompt)

    return response["response"]
